{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce2128a-29b9-4dd9-bd88-b891b54d077f",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering\n",
    "In this notebook, we will further explore the california-housing dataset and prepare the data for modelling. This notebook relies on the output from `1-eda-with-pyspark.ipynb`, which is a parquet file `../data/housing_clean.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fc0144-1a3a-4092-9286-c8bc675403aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/08 06:01:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9194e3-caa7-400f-96fd-d1ca6bee57b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../data/housing_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84409b3e-f350-4c4e-a5d2-f49f26894c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "|longitude|latitude|housingMedianAge|totalRooms|totalBedRooms|population|households|medianIncome|medianHouseValue|\n",
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "|  -122.23|   37.88|            41.0|     880.0|        129.0|     322.0|     126.0|      8.3252|        452600.0|\n",
      "|  -122.22|   37.86|            21.0|    7099.0|       1106.0|    2401.0|    1138.0|      8.3014|        358500.0|\n",
      "|  -122.24|   37.85|            52.0|    1467.0|        190.0|     496.0|     177.0|      7.2574|        352100.0|\n",
      "|  -122.25|   37.85|            52.0|    1274.0|        235.0|     558.0|     219.0|      5.6431|        341300.0|\n",
      "|  -122.25|   37.85|            52.0|    1627.0|        280.0|     565.0|     259.0|      3.8462|        342200.0|\n",
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff10ebf-a3f9-44a4-bd0a-4973768c939b",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137d4e17-097d-43e1-a5a8-d94cd93c323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "|longitude|latitude|housingMedianAge|totalRooms|totalBedRooms|population|households|medianIncome|medianHouseValue|\n",
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "|        0|       0|               0|         0|            0|         0|         0|           0|               0|\n",
      "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13acce9-9431-4816-a73c-8abc29d98981",
   "metadata": {},
   "source": [
    "Get a summary of the data. This will inform our preprocessing tasks and approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76da725-cbb4-4765-bd05-591ce15fe21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          longitude|         latitude|  housingMedianAge|        totalRooms|    totalBedRooms|        population|       households|      medianIncome|  medianHouseValue|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              20640|            20640|             20640|             20640|            20640|             20640|            20640|             20640|             20640|\n",
      "|   mean|-119.56970444871473|35.63186143109965|28.639486434108527|2635.7630813953488|537.8980135658915|1425.4767441860465|499.5396802325581|3.8706710030346416|206855.81690891474|\n",
      "| stddev|  2.003531742932898|2.135952380602968| 12.58555761211163|2181.6152515827944| 421.247905943133|  1132.46212176534|382.3297528316098|1.8998217183639696|115395.61587441359|\n",
      "|    min|            -124.35|            32.54|               1.0|               2.0|              1.0|               3.0|              1.0|            0.4999|           14999.0|\n",
      "|    max|            -114.31|            41.95|              52.0|           39320.0|           6445.0|           35682.0|           6082.0|           15.0001|          500001.0|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52093fd6-9f04-4985-99c7-e09d4039c051",
   "metadata": {},
   "source": [
    "We can derive the following insights from the data:\n",
    "- The range of minimum and maximum for the independent variables is quite large, so we should probably standardise the data\n",
    "- The dependent variable `medianHouseValue` is also quite big, so we should adjust the values\n",
    "- There are some additional attributes which we could add, such as a feature that counts the number of bedrooms per total room or number of rooms per household"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca014b-092c-4e3d-8912-84e2b78a7ecd",
   "metadata": {},
   "source": [
    "### Preprocessing the Target Values\n",
    "We'll deal with this in units of 100,000 to shrink the range and make it easier to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4ddb63-1964-4c99-9004-97659e3523cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526),\n",
       " Row(longitude=-122.22000122070312, latitude=37.86000061035156, housingMedianAge=21.0, totalRooms=7099.0, totalBedRooms=1106.0, population=2401.0, households=1138.0, medianIncome=8.301400184631348, medianHouseValue=3.585)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39327bf-1c73-4ce8-bcdc-68e4b4ab7ec7",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "We will add:\n",
    "- *Rooms per household* number of rooms in households per block group\n",
    "- *Population per household* how many people live in the households per block group\n",
    "- *Bedrooms per room* how many rooms are bedrooms per block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e307a7d-6714-4128-89f4-5c6da887e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43893f0-daf7-481f-8263-a4d6eeed48ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987828f-064d-459c-afaf-48469ff19d69",
   "metadata": {},
   "source": [
    "Now that the features have been created, we want to re-order the columns in the DataFrame so that we isolate the target value before proceeding to standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcf671b0-8070-4140-ae0c-13a9bc7452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4aed24-0e80-4bf3-835a-39fc6241e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n",
      "|medianHouseValue|totalBedRooms|population|households|medianIncome| roomsPerHousehold|populationPerHousehold|    bedroomsPerRoom|\n",
      "+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n",
      "|           4.526|        129.0|     322.0|     126.0|      8.3252| 6.984126984126984|    2.5555555555555554|0.14659090909090908|\n",
      "|           3.585|       1106.0|    2401.0|    1138.0|      8.3014| 6.238137082601054|     2.109841827768014|0.15579659106916466|\n",
      "|           3.521|        190.0|     496.0|     177.0|      7.2574| 8.288135593220339|    2.8022598870056497|0.12951601908657123|\n",
      "|           3.413|        235.0|     558.0|     219.0|      5.6431|5.8173515981735155|     2.547945205479452|0.18445839874411302|\n",
      "|           3.422|        280.0|     565.0|     259.0|      3.8462| 6.281853281853282|    2.1814671814671813| 0.1720958819913952|\n",
      "+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20d6e9-7f08-431c-8f40-d14cc855d53e",
   "metadata": {},
   "source": [
    "### Standardising the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b0199-e426-4d30-9226-e27f5779aa26",
   "metadata": {},
   "source": [
    "There is one more step to go through before normalising the data set; separating the features from the target variable. This boils down to isolating the first column in the DataFrame from the rest of the columns. In this case, we will use the `DenseVector()` function, which creates a local vector that is backed by a double array that represents its entry values. It is what is used to store arrays of values for use in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "346fce97-cb07-4881-855b-d0f66d1d7bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62b2743-fe52-4f34-ac8b-bc1b0736502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|4.526|[129.0,322.0,126....|\n",
      "|3.585|[1106.0,2401.0,11...|\n",
      "|3.521|[190.0,496.0,177....|\n",
      "|3.413|[235.0,558.0,219....|\n",
      "|3.422|[280.0,565.0,259....|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f86eb-0877-4a2f-9dd7-f8de4a739ea6",
   "metadata": {},
   "source": [
    "Now, we can use the `StandardScaler` from the Spark ML library to easily scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3fc2bdd-30b0-420c-9dc3-d2cc9957afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3aa6a-7ae7-4f83-a250-3125ecde8158",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, the data is clean, feature engineered and normalised, meaning it is ready to model on. We will build a regression model on this data using Spark ML in the next notebook. For now, we will save the data to a parquet file to persist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7fda7a-2510-4fca-86f6-0e9b69737492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scaled_df.write.parquet(\"../data/housing_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1503da4-7354-4354-8769-6bf3862cf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae9270-59b9-41e6-9d1b-3fdb25a53956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
