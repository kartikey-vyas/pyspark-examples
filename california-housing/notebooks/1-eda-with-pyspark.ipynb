{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbfc6db-d6ff-4430-b745-fe7d11248251",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "In this notebook, we conduct some basic EDA on the 'California Housing' data set using PySpark. We are using a single machine to test out PySpark functionality for now.\n",
    "\n",
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd7dc0-2612-4954-9a1e-db2d7fff5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ff02d-ee61-48d5-9335-d0d8c0ddc793",
   "metadata": {},
   "source": [
    "### Load and Examine Data\n",
    "> run `bash download_data.sh` in the california-housing directory to retrieve the data from the web\n",
    "\n",
    "We begin by interacting with the data as an RDD (resilient distributed dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fd9094-4de9-4a78-8bc1-623382430afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "rdd = sc.textFile('../data/CaliforniaHousing/cal_housing.data')\n",
    "\n",
    "# Load in the header\n",
    "header = sc.textFile('../data/CaliforniaHousing/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93546f07-4e05-4cb1-97dc-9890343052d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude: continuous.',\n",
       " 'latitude: continuous.',\n",
       " 'housingMedianAge: continuous. ',\n",
       " 'totalRooms: continuous. ',\n",
       " 'totalBedrooms: continuous. ',\n",
       " 'population: continuous. ',\n",
       " 'households: continuous. ',\n",
       " 'medianIncome: continuous. ',\n",
       " 'medianHouseValue: continuous. ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "546953d3-da3a-4b89-a11e-3031342bae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n",
       " '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2d4ed6-b52c-48be-bf64-b7400755e2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-122.230000',\n",
       "  '37.880000',\n",
       "  '41.000000',\n",
       "  '880.000000',\n",
       "  '129.000000',\n",
       "  '322.000000',\n",
       "  '126.000000',\n",
       "  '8.325200',\n",
       "  '452600.000000'],\n",
       " ['-122.220000',\n",
       "  '37.860000',\n",
       "  '21.000000',\n",
       "  '7099.000000',\n",
       "  '1106.000000',\n",
       "  '2401.000000',\n",
       "  '1138.000000',\n",
       "  '8.301400',\n",
       "  '358500.000000']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02e194-1638-43f4-9977-c57ba31b0bc1",
   "metadata": {},
   "source": [
    "RDDs are fine for performing low-level transformations and actions on unstructured data. This means that we don't care about imposing a schema or accessing attributed by name or column. We should use RDDs when we want to manipulate data with functional programming constructs rather than domain specific expressions.\n",
    "\n",
    "### Clean Data and Set Schema\n",
    "\n",
    "Next, we'll convert this RDD to a DataFrame with the `toDF()` method. This is done by first applying a mapping with each of the desired column names corresponding to a different value in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c68091-56ca-4191-983f-f56b43c077ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75425cbc-2dd2-4eb5-ba0e-e8e4c2a2d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
      "|  longitude| latitude|housingMedianAge| totalRooms|totalBedRooms| population| households|medianIncome|medianHouseValue|\n",
      "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
      "|-122.230000|37.880000|       41.000000| 880.000000|   129.000000| 322.000000| 126.000000|    8.325200|   452600.000000|\n",
      "|-122.220000|37.860000|       21.000000|7099.000000|  1106.000000|2401.000000|1138.000000|    8.301400|   358500.000000|\n",
      "|-122.240000|37.850000|       52.000000|1467.000000|   190.000000| 496.000000| 177.000000|    7.257400|   352100.000000|\n",
      "|-122.250000|37.850000|       52.000000|1274.000000|   235.000000| 558.000000| 219.000000|    5.643100|   341300.000000|\n",
      "|-122.250000|37.850000|       52.000000|1627.000000|   280.000000| 565.000000| 259.000000|    3.846200|   342200.000000|\n",
      "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8300592c-c8b5-4ba7-a87c-5a1e466154b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- households: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d8ab1-2da2-4c6a-a1d0-50f01b3461be",
   "metadata": {},
   "source": [
    "This data set's schema needs to be amended to assign the correct data types to each column. These are all floating point values and should be converted as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4abc7576-53df-4231-b9e8-6bb5c1ced6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# assign all columns to list\n",
    "columns = df.columns\n",
    "\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7ff817-5b16-4124-beaf-7aa1118a7d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "654b109b-07cf-4d24-97e6-9ad86988b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          longitude|         latitude|  housingMedianAge|        totalRooms|    totalBedRooms|        population|       households|      medianIncome|  medianHouseValue|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              20640|            20640|             20640|             20640|            20640|             20640|            20640|             20640|             20640|\n",
      "|   mean|-119.56970444871473|35.63186143109965|28.639486434108527|2635.7630813953488|537.8980135658915|1425.4767441860465|499.5396802325581|3.8706710030346416|206855.81690891474|\n",
      "| stddev|  2.003531742932898|2.135952380602968| 12.58555761211163|2181.6152515827944| 421.247905943133|  1132.46212176534|382.3297528316098|1.8998217183639696|115395.61587441359|\n",
      "|    min|            -124.35|            32.54|               1.0|               2.0|              1.0|               3.0|              1.0|            0.4999|           14999.0|\n",
      "|    max|            -114.31|            41.95|              52.0|           39320.0|           6445.0|           35682.0|           6082.0|           15.0001|          500001.0|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef904461-0ff7-44f7-8d8d-cf018854e08e",
   "metadata": {},
   "source": [
    "The data is ready to be further analysed and modelled upon. We'll save it in parquet format and continue working on it in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f8064bf-8082-421c-a476-26d0ac148cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"../data/housing_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "314f1c9a-82b0-40eb-92ad-74be01810350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end spark session for this notebook\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1ee5e-1c54-43ee-930d-956312f20088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
